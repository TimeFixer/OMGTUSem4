{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523fab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\hyperopt\\atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import hyperopt as hp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ea4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls = pd.read_csv('2dataset.csv')\n",
    "data_cls = data_cls.sample(frac=0.1, random_state=42)\n",
    "y_cls = data_cls['RainTomorrow']\n",
    "X_cls = data_cls.drop(['RainTomorrow', 'Date', 'Location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f93d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_cls, y_cls)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aea46e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное количество признаков: 23, и после: 16\n",
      "Размер данных до SMOTE: 8698, и после: 14646\n"
     ]
    }
   ],
   "source": [
    "# Применение PCA (сохранение 95% дисперсии)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Исходное количество признаков: {X_cls.shape[1]}, и после: {X_pca.shape[1]}\")\n",
    "print(f\"Размер данных до SMOTE: {X_cls.shape[0]}, и после: {X_balanced.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdcda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold кросс-валидация\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_pca):\n",
    "    X_train, X_test = X_pca[train_index], X_pca[test_index]\n",
    "    y_train, y_test = y_balanced[train_index], y_balanced[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f832fe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-02 15:38:47,733] A new study created in memory with name: no-name-5456f42b-aa1c-4ea7-af0a-e937835010d2\n",
      "[I 2025-06-02 15:38:48,842] Trial 0 finished with value: 0.7791054967565723 and parameters: {'n_layers': 1, 'n_units_0': 42, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate_init': 0.0021120975048242886}. Best is trial 0 with value: 0.7791054967565723.\n",
      "[I 2025-06-02 15:38:50,142] Trial 1 finished with value: 0.8289518607033117 and parameters: {'n_layers': 1, 'n_units_0': 56, 'solver': 'adam', 'activation': 'tanh', 'learning_rate_init': 0.0020835185156305473}. Best is trial 1 with value: 0.8289518607033117.\n",
      "[I 2025-06-02 15:38:51,007] Trial 2 finished with value: 0.8047115056333219 and parameters: {'n_layers': 1, 'n_units_0': 26, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate_init': 0.06357046128963402}. Best is trial 1 with value: 0.8289518607033117.\n",
      "[I 2025-06-02 15:38:54,654] Trial 3 finished with value: 0.8552406964834415 and parameters: {'n_layers': 3, 'n_units_0': 63, 'n_units_1': 56, 'n_units_2': 54, 'solver': 'lbfgs', 'activation': 'tanh'}. Best is trial 3 with value: 0.8552406964834415.\n",
      "[I 2025-06-02 15:38:57,327] Trial 4 finished with value: 0.8613861386138614 and parameters: {'n_layers': 3, 'n_units_0': 42, 'n_units_1': 57, 'n_units_2': 11, 'solver': 'adam', 'activation': 'relu', 'learning_rate_init': 0.015193994997706607}. Best is trial 4 with value: 0.8613861386138614.\n",
      "[I 2025-06-02 15:38:58,347] Trial 5 finished with value: 0.8187094571526118 and parameters: {'n_layers': 1, 'n_units_0': 45, 'solver': 'lbfgs', 'activation': 'tanh'}. Best is trial 4 with value: 0.8613861386138614.\n",
      "[I 2025-06-02 15:39:00,865] Trial 6 finished with value: 0.8180266302492318 and parameters: {'n_layers': 2, 'n_units_0': 61, 'n_units_1': 55, 'solver': 'adam', 'activation': 'tanh', 'learning_rate_init': 0.05884810495354834}. Best is trial 4 with value: 0.8613861386138614.\n",
      "[I 2025-06-02 15:39:03,425] Trial 7 finished with value: 0.8436326391259815 and parameters: {'n_layers': 3, 'n_units_0': 38, 'n_units_1': 59, 'n_units_2': 38, 'solver': 'lbfgs', 'activation': 'relu'}. Best is trial 4 with value: 0.8613861386138614.\n",
      "[I 2025-06-02 15:39:06,947] Trial 8 finished with value: 0.8228064185728917 and parameters: {'n_layers': 3, 'n_units_0': 18, 'n_units_1': 46, 'n_units_2': 93, 'solver': 'lbfgs', 'activation': 'tanh'}. Best is trial 4 with value: 0.8613861386138614.\n",
      "[I 2025-06-02 15:39:09,025] Trial 9 finished with value: 0.8306589279617617 and parameters: {'n_layers': 2, 'n_units_0': 29, 'n_units_1': 65, 'solver': 'lbfgs', 'activation': 'tanh'}. Best is trial 4 with value: 0.8613861386138614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Results:\n",
      "Best parameters: {'n_layers': 3, 'n_units_0': 42, 'n_units_1': 57, 'n_units_2': 11, 'solver': 'adam', 'activation': 'relu', 'learning_rate_init': 0.015193994997706607}\n",
      "Best accuracy: 0.8614\n"
     ]
    }
   ],
   "source": [
    "def objective_optuna(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f'n_units_{i}', 10, 100))\n",
    "    \n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd', 'lbfgs'])\n",
    "    params = {\n",
    "        'hidden_layer_sizes': tuple(layers),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "        'solver': solver,\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-5, 1e-1, log=True) if solver != 'lbfgs' else 0.001,\n",
    "        'max_iter': 100,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = MLPClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_optuna, n_trials=10)\n",
    "optuna_params = study.best_params\n",
    "optuna_score = study.best_value\n",
    "\n",
    "print(\"\\nOptuna Results:\")\n",
    "print(f\"Best parameters: {optuna_params}\")\n",
    "print(f\"Best accuracy: {optuna_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dec11df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomizedSearchCV Results:\n",
      "Best parameters: {'solver': 'adam', 'max_iter': 100, 'learning_rate_init': np.float64(0.0042292428743894986), 'hidden_layer_sizes': (100, 50), 'activation': 'relu'}\n",
      "Best accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "# 2. RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (50, 25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd', 'lbfgs'],\n",
    "    'learning_rate_init': np.logspace(-5, -1, 100),\n",
    "    'max_iter': [100]\n",
    "}\n",
    "\n",
    "model = MLPClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, \n",
    "                                 n_iter=10, cv=3, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "random_params = random_search.best_params_\n",
    "random_score = random_search.best_score_\n",
    "\n",
    "print(\"\\nRandomizedSearchCV Results:\")\n",
    "print(f\"Best parameters: {random_params}\")\n",
    "print(f\"Best accuracy: {random_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24dd90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:06<00:00,  9.35s/trial, best loss: -0.8163950143815916]\n",
      "\n",
      "Hyperopt Results:\n",
      "Best parameters: {'activation': np.int64(0), 'hidden_layer_sizes': np.int64(3), 'learning_rate_init': np.float64(0.018206606977761358)}\n",
      "Best accuracy: 0.8164\n"
     ]
    }
   ],
   "source": [
    "# 3. Hyperopt\n",
    "def objective_hyperopt(params):\n",
    "    learning_rate = params['learning_rate_init'] if params['solver'] != 'lbfgs' else 0.001\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "        activation=params['activation'],\n",
    "        solver=params['solver'],\n",
    "        learning_rate_init=learning_rate,\n",
    "        max_iter=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return -accuracy_score(y_test, y_pred)\n",
    "\n",
    "space = {\n",
    "    'hidden_layer_sizes': hp.hp.choice('hidden_layer_sizes', \n",
    "                                  [(50,), (100,), (50, 50), (100, 50), (50, 25)]),\n",
    "    'activation': hp.hp.choice('activation', ['relu', 'tanh']),\n",
    "    'solver': hp.hp.choice('solver', ['adam', 'sgd', 'lbfgs']),\n",
    "    'learning_rate_init': hp.hp.loguniform('learning_rate_init', -5, -1)\n",
    "}\n",
    "\n",
    "trials = hp.Trials()\n",
    "hyperopt_params = hp.fmin(fn=objective_hyperopt,\n",
    "                      space=space,\n",
    "                      algo=hp.tpe.suggest,\n",
    "                      max_evals=10,\n",
    "                      trials=trials)\n",
    "hyperopt_score = -min(trials.losses())\n",
    "\n",
    "print(\"\\nHyperopt Results:\")\n",
    "print(f\"Best parameters: {hyperopt_params}\")\n",
    "print(f\"Best accuracy: {hyperopt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Базовый класс для слоев\n",
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Аффинный слой (WX + b)\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.b = np.zeros((1, output_size))\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.W) + self.b\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.W.T)\n",
    "        grad_W = np.dot(self.input.T, grad_output)\n",
    "        grad_b = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        return grad_input, {'W': grad_W, 'b': grad_b}\n",
    "\n",
    "# Классы функций активации\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * (self.input > 0)\n",
    "        return grad_input, {}\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * (1 - np.tanh(self.input) ** 2)\n",
    "        return grad_input, {}\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        sigmoid = 1 / (1 + np.exp(-self.input))\n",
    "        grad_input = grad_output * sigmoid * (1 - sigmoid)\n",
    "        return grad_input, {}\n",
    "\n",
    "# Классы функций потерь\n",
    "class Loss:\n",
    "    def compute(self, y_pred, y_true):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def gradient(self, y_pred, y_true):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MSELoss(Loss):\n",
    "    def compute(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def gradient(self, y_pred, y_true):\n",
    "        return 2 * (y_pred - y_true) / y_pred.shape[0]\n",
    "\n",
    "class CrossEntropyLoss(Loss):\n",
    "    def compute(self, y_pred, y_true):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def gradient(self, y_pred, y_true):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred) * y_pred.shape[0])\n",
    "\n",
    "# Класс оптимизатора\n",
    "class SGDOptimizer:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def update(self, layer, gradients):\n",
    "        if hasattr(layer, 'W') and 'W' in gradients:\n",
    "            layer.W -= self.lr * gradients['W']\n",
    "            layer.b -= self.lr * gradients['b']\n",
    "\n",
    "# Класс нейронной сети\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss_fn, optimizer):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        gradients = []\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_input, grad_params = layer.backward(grad_output)\n",
    "            gradients.append(grad_params)\n",
    "            grad_output = grad_input\n",
    "        return list(reversed(gradients))\n",
    "    \n",
    "    def train(self, x, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            # Прямое распространение\n",
    "            y_pred = self.forward(x)\n",
    "            \n",
    "            # Вычисление потерь\n",
    "            loss = self.loss_fn.compute(y_pred, y)\n",
    "            \n",
    "            # Обратное распространение\n",
    "            grad_output = self.loss_fn.gradient(y_pred, y)\n",
    "            gradients = self.backward(grad_output)\n",
    "            \n",
    "            # Обновление весов\n",
    "            for layer, grad in zip(self.layers, gradients):\n",
    "                self.optimizer.update(layer, grad)\n",
    "                \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.forward(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
